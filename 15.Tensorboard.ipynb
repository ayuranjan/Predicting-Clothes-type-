{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a font-end web interface that essentially reads data from a file and displays it. To use TensorBoard our task is to get the data we want displayed saved to a file that TensorBoard can read.\n",
    "\n",
    "To make this easy for us, PyTorch has created a utility class called SummaryWriter. To get access to this class we use the following import:\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "Once we have imported the class, we can create an instance of the class that we'll then use to get the data out of our program and onto the file system where it can then be consumed by TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_set\n",
    "    ,batch_size=10\n",
    "    ,shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)#linear, dense, and fully connected layer all are same\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self, t):\n",
    "    # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "\n",
    "network = Network()\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "tb.close()\n",
    "\n",
    "#This code creates a SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TensorBoard\n",
    "To launch TensorBoard, we need to run the tensorboard command at our terminal. This will launch a local server that will serve the TensorBoard UI and the the data our SummaryWriter wrote to disk.\n",
    "\n",
    "By default, the PyTorch SummaryWriter object writes the data to disk in a directory called ./runs that is created in the current working directory.\n",
    "\n",
    "When we run the tensorboard command, we pass an argument that tells tensorboard where the data is. So it's like this:\n",
    "\n",
    "tensorboard --logdir=runs\n",
    "The TensorBoard server will launch and be listening for http requests on port 6006. These details will be displayed in the console.\n",
    "\n",
    "Access the TensorBoard UI by browsing to:\n",
    "\n",
    "http://localhost:6006\n",
    "\n",
    "## TensorBoard Histograms and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 46637 loss: 348.67361275851727\n",
      "epoch 1 total_correct: 51120 loss: 239.23234041035175\n",
      "epoch 2 total_correct: 51864 loss: 217.5779768228531\n",
      "epoch 3 total_correct: 52276 loss: 209.0313512980938\n",
      "epoch 4 total_correct: 52432 loss: 203.20759464800358\n",
      "epoch 5 total_correct: 52620 loss: 197.5764152854681\n",
      "epoch 6 total_correct: 52879 loss: 194.0111052542925\n",
      "epoch 7 total_correct: 52754 loss: 192.17617286741734\n",
      "epoch 8 total_correct: 53186 loss: 184.22582198679447\n",
      "epoch 9 total_correct: 53121 loss: 186.3282471075654\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "        \n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "    tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "    tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "    tb.add_histogram(\n",
    "            'conv1.weight.grad'\n",
    "            ,network.conv1.weight.grad\n",
    "            ,epoch\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will add these values to TensorBoard. The values even update in real-time as the network trains.\n",
    "\n",
    "The real power of TensorBoard is its out-of-the-box capability of comparing multiple runs. This allows us to rapidly experiment by varying the hyperparameter values and comparing runs to see which parameters are working best.\n",
    "## Naming the Training Runs for TensorBoard\n",
    "To take advantage of TensorBoard comparison capabilities, we need to do multiple runs and name each run in such a way that we can identify it uniquely.\n",
    "\n",
    "With PyTorch's SummaryWriter, a run starts when the writer object instance is created and ends when the writer instance is closed or goes out of scope.\n",
    "\n",
    "To uniquely identify each run, we can either set the file name of the run directly, or pass a comment string to the constructor that will be appended to the auto-generated file name.\n",
    "### Choosing a Name for the Run\n",
    "\n",
    "One way to name the run is to add the parameter names and values as a comment for the run. This will allow us to see how each parameter value stacks up with the others later when we are reviewing the runs inside TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of hyperparamter values on which our model would be tested .\n",
    "batch_size_list = [100, 1000, 10000]\n",
    "lr_list = [.01, .001, .0001, .00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 47371 loss: 33295.010417699814\n",
      "epoch 1 total_correct: 51277 loss: 23253.64562869072\n",
      "epoch 2 total_correct: 51949 loss: 21531.316296756268\n",
      "epoch 3 total_correct: 52303 loss: 20488.59374523163\n",
      "epoch 4 total_correct: 52578 loss: 19908.43445956707\n",
      "epoch 5 total_correct: 52674 loss: 19767.661049962044\n",
      "epoch 6 total_correct: 52838 loss: 19177.610696852207\n",
      "epoch 7 total_correct: 52749 loss: 19427.634966373444\n",
      "epoch 8 total_correct: 53126 loss: 18471.25412374735\n",
      "epoch 9 total_correct: 53226 loss: 18369.066247344017\n",
      "epoch 0 total_correct: 41507 loss: 48938.33390176296\n",
      "epoch 1 total_correct: 48013 loss: 31853.487139940262\n",
      "epoch 2 total_correct: 50184 loss: 26767.59902536869\n",
      "epoch 3 total_correct: 51257 loss: 23788.830955326557\n",
      "epoch 4 total_correct: 51884 loss: 21942.797024548054\n",
      "epoch 5 total_correct: 52424 loss: 20649.516186118126\n",
      "epoch 6 total_correct: 52814 loss: 19634.12436544895\n",
      "epoch 7 total_correct: 53144 loss: 18772.239856421947\n",
      "epoch 8 total_correct: 53368 loss: 18020.28916850686\n",
      "epoch 9 total_correct: 53644 loss: 17349.96448531747\n",
      "epoch 0 total_correct: 31189 loss: 81889.38664197922\n",
      "epoch 1 total_correct: 42185 loss: 47705.03454208374\n",
      "epoch 2 total_correct: 43628 loss: 43123.61318171024\n",
      "epoch 3 total_correct: 44627 loss: 40486.525574326515\n",
      "epoch 4 total_correct: 45361 loss: 38640.84624052048\n",
      "epoch 5 total_correct: 45895 loss: 37156.20751082897\n",
      "epoch 6 total_correct: 46359 loss: 35886.985567212105\n",
      "epoch 7 total_correct: 46803 loss: 34784.73687171936\n",
      "epoch 8 total_correct: 47224 loss: 33796.470576524734\n",
      "epoch 9 total_correct: 47625 loss: 32896.48098349571\n",
      "epoch 0 total_correct: 7308 loss: 137465.84730148315\n",
      "epoch 1 total_correct: 16559 loss: 128452.31205224991\n",
      "epoch 2 total_correct: 28057 loss: 103154.86876964569\n",
      "epoch 3 total_correct: 34306 loss: 81837.48980760574\n",
      "epoch 4 total_correct: 35971 loss: 69323.66287708282\n",
      "epoch 5 total_correct: 37207 loss: 62352.159279584885\n",
      "epoch 6 total_correct: 38208 loss: 58307.34834074974\n",
      "epoch 7 total_correct: 39014 loss: 55697.05035686493\n",
      "epoch 8 total_correct: 39767 loss: 53781.48531913757\n",
      "epoch 9 total_correct: 40391 loss: 52273.12148809433\n",
      "epoch 0 total_correct: 36958 loss: 60269.775688648224\n",
      "epoch 1 total_correct: 46429 loss: 34977.95704007149\n",
      "epoch 2 total_correct: 48852 loss: 29476.134330034256\n",
      "epoch 3 total_correct: 50119 loss: 26559.79010462761\n",
      "epoch 4 total_correct: 51448 loss: 23755.744069814682\n",
      "epoch 5 total_correct: 51986 loss: 21928.669542074203\n",
      "epoch 6 total_correct: 52515 loss: 20477.963626384735\n",
      "epoch 7 total_correct: 52779 loss: 19515.101730823517\n",
      "epoch 8 total_correct: 53228 loss: 18503.26755642891\n",
      "epoch 9 total_correct: 53312 loss: 18006.818532943726\n",
      "epoch 0 total_correct: 26446 loss: 95956.42566680908\n",
      "epoch 1 total_correct: 41952 loss: 48177.41930484772\n",
      "epoch 2 total_correct: 44353 loss: 41597.085535526276\n",
      "epoch 3 total_correct: 45596 loss: 37516.570925712585\n",
      "epoch 4 total_correct: 46498 loss: 34943.06695461273\n",
      "epoch 5 total_correct: 47387 loss: 33078.94027233124\n",
      "epoch 6 total_correct: 48048 loss: 31654.345721006393\n",
      "epoch 7 total_correct: 48638 loss: 30413.40982913971\n",
      "epoch 8 total_correct: 49203 loss: 29330.93199133873\n",
      "epoch 9 total_correct: 49634 loss: 28356.985956430435\n",
      "epoch 0 total_correct: 7557 loss: 137645.4563140869\n",
      "epoch 1 total_correct: 12927 loss: 132337.1660709381\n",
      "epoch 2 total_correct: 30303 loss: 109841.20273590088\n",
      "epoch 3 total_correct: 34750 loss: 78642.02725887299\n",
      "epoch 4 total_correct: 38164 loss: 62584.41472053528\n",
      "epoch 5 total_correct: 40131 loss: 55273.96911382675\n",
      "epoch 6 total_correct: 41246 loss: 51442.81870126724\n",
      "epoch 7 total_correct: 41826 loss: 49248.13324213028\n",
      "epoch 8 total_correct: 42244 loss: 47736.38188838959\n",
      "epoch 9 total_correct: 42633 loss: 46516.21466875076\n",
      "epoch 0 total_correct: 6000 loss: 138258.9828968048\n",
      "epoch 1 total_correct: 6000 loss: 138191.04886054993\n",
      "epoch 2 total_correct: 6000 loss: 138112.6744747162\n",
      "epoch 3 total_correct: 6000 loss: 138014.03141021729\n",
      "epoch 4 total_correct: 6000 loss: 137883.2528591156\n",
      "epoch 5 total_correct: 6000 loss: 137712.02993392944\n",
      "epoch 6 total_correct: 6000 loss: 137478.4698486328\n",
      "epoch 7 total_correct: 6000 loss: 137155.31635284424\n",
      "epoch 8 total_correct: 6434 loss: 136769.42777633667\n",
      "epoch 9 total_correct: 9363 loss: 136273.01907539368\n",
      "epoch 0 total_correct: 12307 loss: 128682.91020393372\n",
      "epoch 1 total_correct: 25815 loss: 88742.46120452881\n",
      "epoch 2 total_correct: 35153 loss: 65912.64128684998\n",
      "epoch 3 total_correct: 40061 loss: 53624.36056137085\n",
      "epoch 4 total_correct: 42449 loss: 47067.71671772003\n",
      "epoch 5 total_correct: 43782 loss: 42263.08882236481\n",
      "epoch 6 total_correct: 44551 loss: 39538.9187335968\n",
      "epoch 7 total_correct: 45566 loss: 36573.04108142853\n",
      "epoch 8 total_correct: 46187 loss: 34790.68100452423\n",
      "epoch 9 total_correct: 46972 loss: 33142.353892326355\n",
      "epoch 0 total_correct: 6481 loss: 137682.11841583252\n",
      "epoch 1 total_correct: 12032 loss: 134592.4997329712\n",
      "epoch 2 total_correct: 18918 loss: 126848.91939163208\n",
      "epoch 3 total_correct: 23244 loss: 111644.17386054993\n",
      "epoch 4 total_correct: 32859 loss: 90311.68103218079\n",
      "epoch 5 total_correct: 36806 loss: 69737.33067512512\n",
      "epoch 6 total_correct: 37574 loss: 59553.000926971436\n",
      "epoch 7 total_correct: 39143 loss: 55103.84500026703\n",
      "epoch 8 total_correct: 40343 loss: 51945.719718933105\n",
      "epoch 9 total_correct: 41300 loss: 49434.67140197754\n",
      "epoch 0 total_correct: 6000 loss: 138299.86333847046\n",
      "epoch 1 total_correct: 6000 loss: 138081.62450790405\n",
      "epoch 2 total_correct: 6000 loss: 137862.4963760376\n",
      "epoch 3 total_correct: 6000 loss: 137634.5157623291\n",
      "epoch 4 total_correct: 6730 loss: 137382.84587860107\n",
      "epoch 5 total_correct: 9293 loss: 137088.885307312\n",
      "epoch 6 total_correct: 10910 loss: 136740.4317855835\n",
      "epoch 7 total_correct: 11566 loss: 136317.21019744873\n",
      "epoch 8 total_correct: 12268 loss: 135821.99573516846\n",
      "epoch 9 total_correct: 14322 loss: 135231.39238357544\n",
      "epoch 0 total_correct: 6000 loss: 138233.1418991089\n",
      "epoch 1 total_correct: 6000 loss: 138221.2734222412\n",
      "epoch 2 total_correct: 6000 loss: 138209.2809677124\n",
      "epoch 3 total_correct: 6000 loss: 138197.124004364\n",
      "epoch 4 total_correct: 6000 loss: 138184.7620010376\n",
      "epoch 5 total_correct: 6000 loss: 138172.38569259644\n",
      "epoch 6 total_correct: 6000 loss: 138159.42287445068\n",
      "epoch 7 total_correct: 6000 loss: 138146.1238861084\n",
      "epoch 8 total_correct: 6000 loss: 138132.586479187\n",
      "epoch 9 total_correct: 6000 loss: 138118.67475509644\n"
     ]
    }
   ],
   "source": [
    "#We create 2 loops for 2 hyperparamter we gonna tune and check\n",
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        #batch_size = 100\n",
    "        #lr = 0.01\n",
    "        #because we want to change or tune these hyperparameter \n",
    "        network = Network()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment = f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment = comment)\n",
    "        #to uniquely identify the name of the run ... the summarywritter will append the comment to name of the run\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(10):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "\n",
    "            for batch in train_loader: # Get Batch\n",
    "                images, labels = batch \n",
    "\n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() # Calculate Gradients\n",
    "                optimizer.step() # Update Weights\n",
    "\n",
    "                # initially total_loss += loss.item()\n",
    "                total_loss += loss.item() * batch_size\n",
    "                #why changed ? to compare the different runs as batch size would be a hyperparamter. so we can't just compare loss\n",
    "                total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "            tb.add_scalar('Loss', total_loss, epoch)\n",
    "            tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "            tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "            tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "            tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "            tb.add_histogram(\n",
    "                    'conv1.weight.grad'\n",
    "                    ,network.conv1.weight.grad\n",
    "                    ,epoch\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"epoch\", epoch, \n",
    "                \"total_correct:\", total_correct, \n",
    "                \"loss:\", total_loss\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Network Parameters & Gradients to TensorBoard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "We did this using the code below:\n",
    "\n",
    "tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "\n",
    "Now, we've enhanced this by adding these values for all of our layers using the loop below:\n",
    "\n",
    "for name, weight in network.named_parameters():\n",
    "    tb.add_histogram(name, weight, epoch)\n",
    "    tb.add_histogram(f'{name}.grad', weight.grad, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Hyperparameters Without Nesting\n",
    "\n",
    "This is cool. However, what if we want to add a third or even a forth parameter to iterate on? We'll, this is going to get messy with many nested for-loops.\n",
    "\n",
    "If we have a list of parameters, we can package them up into a set for each of our runs using the Cartesian product. For this we'll use the product function from the itertools library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a dictionary that contains parameters as keys and parameter values we want to use as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [100, 1000]\n",
    "    ,shuffle = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a list of iterables that we can pass to the product functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_values = [v for v in parameters.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [100, 1000], [True, False]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have three lists of parameter values. After we take the Cartesian product of these three lists, we'll have a set of parameter values for each of our runs\n",
    "\n",
    "now we can iterate over each set of parameters using a single for-loop. All we have to do is unpack the set using sequence unpacking. It looks like this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for lr, batch_size, shuffle in product(*param_values): \n",
    "    comment = f' batch_size={batch_size} lr={lr} shuffle={shuffle}'\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set\n",
    "        ,batch_size=batch_size\n",
    "        ,shuffle=shuffle \n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        network.parameters(), lr=lr\n",
    "    )\n",
    "\n",
    "    # Rest of training process given the set of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice the * operator. This is a special way in Python to unpack a list into a set of arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Run Builder\n",
    "we’ll code a RunBuilder class that will allow us to generate multiple runs with varying parameters.\n",
    "\n",
    "We’ll start with our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing to note about using this class is that it has a static method called get_runs(). This method will get the runs for us that it builds based on the parameters we pass in.\n",
    "\n",
    "Let’s define some parameters now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [1000, 10000]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these runs, we just call the get_runs() function of the RunBuilder class, passing in the parameters we’d like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " runs = RunBuilder.get_runs(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(lr=0.01, batch_size=1000),\n",
       " Run(lr=0.01, batch_size=10000),\n",
       " Run(lr=0.001, batch_size=1000),\n",
       " Run(lr=0.001, batch_size=10000)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs\n",
    "#this contains all the possible combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access an individual run by indexing into the list like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(lr=0.01, batch_size=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the run is object is a tuple with named attributes, we can access the values using dot notation like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 1000\n"
     ]
    }
   ],
   "source": [
    "run = runs[0]\n",
    "print(run.lr, run.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(lr=0.01, batch_size=1000) 0.01 1000\n",
      "Run(lr=0.01, batch_size=10000) 0.01 10000\n",
      "Run(lr=0.001, batch_size=1000) 0.001 1000\n",
      "Run(lr=0.001, batch_size=10000) 0.001 10000\n"
     ]
    }
   ],
   "source": [
    "# we can iterate over the runs cleanly like so:\n",
    "\n",
    "for run in runs:\n",
    "    print(run, run.lr, run.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets look inside the run builder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr', 'batch_size'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #we can get the keys of the dictionary by :- \n",
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([[0.01, 0.001], [1000, 10000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #we can get the values of the dictionary by :- \n",
    "params.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " #we use these keys and values for what comes next. We’ll start with the keys.\n",
    "Run = namedtuple('Run', params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Run class is used to encapsulate the data for each of our runs\n",
    "\n",
    "The field names of this class are set by the list of names passed to the constructor. First, we are passing the class name. Then, we are passing the field names, and in our case, we are passing the list of keys from our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for v in product(*params.values()):\n",
    "    runs.append(Run(*v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a list called runs. Then, we use the product() function from itertools to create the Cartesian product using the values for each parameter inside our dictionary. This gives us a set of ordered pairs that define our runs. We iterate over these adding a run to the runs list for each one.\n",
    "\n",
    "For each value in the Cartesian product we have an ordered tuples. The Cartesian product gives us every ordered pair so we have all possible order pairs of learning rates and batch sizes. When we pass the tuple to the Run constructor, we use the * operator to tell the constructor to accept the tuple values as arguments opposed to the tuple itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the get_runs() method is static, we can call it using the class itself. We don’t need an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 37780 loss: 58599.32154417038\n",
      "epoch 1 total_correct: 47574 loss: 32896.0095345974\n",
      "epoch 2 total_correct: 50360 loss: 26575.594872236252\n",
      "epoch 3 total_correct: 51531 loss: 23226.498186588287\n",
      "epoch 4 total_correct: 52153 loss: 21278.14558148384\n",
      "epoch 0 total_correct: 15934 loss: 122171.53668403625\n",
      "epoch 1 total_correct: 29269 loss: 77561.65146827698\n",
      "epoch 2 total_correct: 37412 loss: 60099.947452545166\n",
      "epoch 3 total_correct: 40782 loss: 49917.40643978119\n",
      "epoch 4 total_correct: 43081 loss: 44133.17859172821\n",
      "epoch 0 total_correct: 29046 loss: 92105.70079088211\n",
      "epoch 1 total_correct: 40240 loss: 51495.10437250137\n",
      "epoch 2 total_correct: 43045 loss: 43882.4542760849\n",
      "epoch 3 total_correct: 44734 loss: 39440.139174461365\n",
      "epoch 4 total_correct: 45993 loss: 36319.87351179123\n",
      "epoch 0 total_correct: 6682 loss: 137506.74724578857\n",
      "epoch 1 total_correct: 12265 loss: 133907.208442688\n",
      "epoch 2 total_correct: 21892 loss: 124497.26819992065\n",
      "epoch 3 total_correct: 26896 loss: 106004.64463233948\n",
      "epoch 4 total_correct: 34532 loss: 83296.6685295105\n"
     ]
    }
   ],
   "source": [
    "for run in RunBuilder.get_runs(params):\n",
    "        comment = f'-{run}'\n",
    "        network = Network()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        #comment = f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment = comment)\n",
    "        #to uniquely identify the name of the run ... the summarywritter will append the comment to name of the run\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(5):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "\n",
    "            for batch in train_loader: # Get Batch\n",
    "                images, labels = batch \n",
    "\n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() # Calculate Gradients\n",
    "                optimizer.step() # Update Weights\n",
    "\n",
    "                # initially total_loss += loss.item()\n",
    "                total_loss += loss.item() * run.batch_size\n",
    "                #why changed ? to compare the different runs as batch size would be a hyperparamter. so we can't just compare loss\n",
    "                total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "            tb.add_scalar('Loss', total_loss, epoch)\n",
    "            tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "            tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "            tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "            tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "            tb.add_histogram(\n",
    "                    'conv1.weight.grad'\n",
    "                    ,network.conv1.weight.grad\n",
    "                    ,epoch\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"epoch\", epoch, \n",
    "                \"total_correct:\", total_correct, \n",
    "                \"loss:\", total_loss\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader num_workers \n",
    "we will see how we can speed up the neural network training process by utilizing the multiple process capabilities of the PyTorch DataLoader class.\n",
    "\n",
    "The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process.\n",
    "\n",
    "Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. The batches are loaded using additional worker processes and are queued up in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [1000, 10000]\n",
    "    ,num_workers = [1,2,4,8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 37212 loss: 58812.82848119736\n",
      "epoch 1 total_correct: 48630 loss: 30188.325732946396\n",
      "epoch 0 total_correct: 36806 loss: 59732.29068517685\n",
      "epoch 1 total_correct: 47691 loss: 31755.951434373856\n",
      "epoch 0 total_correct: 38101 loss: 57306.09983205795\n",
      "epoch 1 total_correct: 48455 loss: 30193.024933338165\n",
      "epoch 0 total_correct: 38005 loss: 58287.00304031372\n",
      "epoch 1 total_correct: 48141 loss: 31524.434506893158\n",
      "epoch 0 total_correct: 15734 loss: 126101.9766330719\n",
      "epoch 1 total_correct: 31145 loss: 78653.46789360046\n",
      "epoch 0 total_correct: 12641 loss: 127871.3583946228\n",
      "epoch 1 total_correct: 30506 loss: 76154.60276603699\n",
      "epoch 0 total_correct: 13269 loss: 128005.11002540588\n",
      "epoch 1 total_correct: 29120 loss: 82087.17823028564\n",
      "epoch 0 total_correct: 11345 loss: 130399.26767349243\n",
      "epoch 1 total_correct: 23567 loss: 95575.30760765076\n"
     ]
    }
   ],
   "source": [
    "for run in RunBuilder.get_runs(params):\n",
    "        comment = f'-{run}'\n",
    "        network = Network()\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size,num_workers = run.num_workers )\n",
    "        optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        #comment = f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment = comment)\n",
    "        #to uniquely identify the name of the run ... the summarywritter will append the comment to name of the run\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(2):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "\n",
    "            for batch in train_loader: # Get Batch\n",
    "                images, labels = batch \n",
    "\n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() # Calculate Gradients\n",
    "                optimizer.step() # Update Weights\n",
    "\n",
    "                # initially total_loss += loss.item()\n",
    "                total_loss += loss.item() * run.batch_size\n",
    "                #why changed ? to compare the different runs as batch size would be a hyperparamter. so we can't just compare loss\n",
    "                total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "            tb.add_scalar('Loss', total_loss, epoch)\n",
    "            tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "            tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "            tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "            tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "            tb.add_histogram(\n",
    "                    'conv1.weight.grad'\n",
    "                    ,network.conv1.weight.grad\n",
    "                    ,epoch\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                \"epoch\", epoch, \n",
    "                \"total_correct:\", total_correct, \n",
    "                \"loss:\", total_loss\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
