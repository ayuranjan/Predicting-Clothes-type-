{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)#linear, dense, and fully connected layer all are same\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we need to implement our network's forward() method, and then, finally, we'll be ready to train our model.\n",
    "\n",
    "Precap - the last point for creating a neural network is :-\n",
    "\n",
    "Use the network's layer attributes as well nn.functional API operations to define the network's forward pass.\n",
    "\n",
    "The forward() method is the actual network transformation. The forward method is the mapping that maps an input tensor to a prediction output tensor. Let's see how this is done.\n",
    "\n",
    "\n",
    "We have two convolutional layers and three Linear layers. If we count the input layer, this gives us a network with a total of six layers.\n",
    "\n",
    "## Implementing the forward() method\n",
    "Let's code this up. We'll kick things off with the input layer.\n",
    "### Input layer #1\n",
    "The input layer of any neural network is determined by the input data. For example, if our input tensor contains three elements, our network would have three nodes contained in its input layer.\n",
    "\n",
    "For this reason, we can think of the input layer as the identity transformation. Mathematically, this is the function,\n",
    "f(x)=x\n",
    "\n",
    "### (1) input layer\n",
    "t = t\n",
    "\n",
    "## Hidden convolutional layers: Layers #2 and #3\n",
    "When want to call the forward() method of a nn.Module instance, we call the actual instance instead of calling the forward() method directly.\n",
    "\n",
    "Instead of doing this self.conv1.forward(tensor), we do this self.conv1(tensor).\n",
    "\n",
    "### (2) hidden conv layer\n",
    "t = self.conv1(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "### (3) hidden conv layer\n",
    "t = self.conv2(t)\n",
    "t = F.relu(t)\n",
    "t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "Each of these layers is comprised of a collection of weights (data) and a collection operations (code). The weights are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are just pure operations. Neither of these have weights, and this is why we call them directly from the nn.functional API.\n",
    "\n",
    "Sometimes we may see pooling operations referred to as pooling layers. Sometimes we may even hear activation operations called activation layers.\n",
    "\n",
    "However, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and activation functions do not have weights, we will refer to them as operations and view them as being added to the collection of layer operations.\n",
    "\n",
    "For example, we'll say that the second layer in our network is a convolutional layer that contains a collection of weights, and preforms three operations, a convolution operation, the relu activation operation, and the max pooling operation.\n",
    "\n",
    "Note that the rules and terminology here are not strict. This is just one way to describe a network. There are other ways to express these ideas. The main thing we need to be aware of is which operations are defined using weights and which ones don't use any weights\n",
    "\n",
    "Mathematically, the entire network is just a composition of functions, and a composition of functions is a function itself. So a network is just a function. All the terms like layers, activation functions, and weights, are just used to help describe the different parts.\n",
    "\n",
    "## Hidden linear layers: Layers #4 and #5\n",
    "Before we pass our input to the first hidden linear layer, we must reshape() or flatten our tensor\n",
    "\n",
    "### (4) hidden linear layer\n",
    "t = t.reshape(-1, 12 * 4 * 4)\n",
    "t = self.fc1(t)\n",
    "t = F.relu(t)\n",
    "\n",
    "### (5) hidden linear layer\n",
    "t = self.fc2(t)\n",
    "t = F.relu(t)\n",
    "\n",
    "We saw in the post on CNN weights that the number 12 in the reshaping operation is determined by the number of output channels coming from the previous convolutional layer.\n",
    "\n",
    "However, the 4 * 4 was left as an open question. Let's reveal the answer now. The 4 * 4 is actually the height and width of each of the 12 output channels.\n",
    "\n",
    "We started with a 1 x 28 x 28 input tensor. This gives a single color channel, 28 x 28 image, and by the time our tensor arrives at the first linear layer, the dimensions have changed.\n",
    "\n",
    "The height and width dimensions have been reduced from 28 x 28 to 4 x 4 by the convolution and pooling operations.\n",
    "\n",
    "Convolution and pooling operations are reduction operations on the height and width dimensions. We'll see how this works and see a formula for calculating these reductions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the tensor is reshaped, we pass the flattened tensor to the linear layer and pass this result to the relu() activation function.\n",
    "\n",
    "## Output layer #6\n",
    "\n",
    "# (6) output layer\n",
    "t = self.out(t)\n",
    "#t = F.softmax(t, dim=1)\n",
    "\n",
    "The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1.\n",
    "\n",
    "However, in our case, we won't use softmax() because the loss function that we'll use, F.cross_entropy(), implicitly performs the softmax() operation on its input, so we'll just return the result of the last linear transformation.\n",
    "\n",
    "The implication of this is that our network will be trained using the softmax operation but will not need to compute the additional operation when the network is used for inference after the training process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)#linear, dense, and fully connected layer all are same\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self, t):\n",
    "    # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
