{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Package        \t                        Description\n",
    "torch\t                    The top-level PyTorch package and tensor library.\n",
    "torch.nn\t                A subpackage that contains modules and extensible classes for building neural networks.\n",
    "torch.optim\t                A subpackage that contains standard optimization operations like SGD and Adam.\n",
    "torch.nn.functional\t        A functional interface that contains typical operations used for building neural networks                             like loss functions and convolutions.\n",
    "torchvision            \t    A package that provides access to popular datasets, model architectures, and image                                     transformations for computer vision.\n",
    "torchvision.transforms\t    An interface that contains common transforms for image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our ultimate goal when preparing our data is to do the following (ETL):\n",
    "\n",
    "#### Extract – Get the Fashion-MNIST image data from the source.\n",
    "#### Transform – Put our data into tensor form.\n",
    "#### Load – Put our data into an object to make it easily accessible.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For these purposes, PyTorch provides us with two classes:\n",
    "\n",
    "Class\t                                        Description\n",
    "torch.utils.data.Dataset\t           An abstract class for representing a dataset.\n",
    "torch.utils.data.DataLoader\t           Wraps a dataset and provides access to the underlying data.\n",
    "\n",
    "An abstract class is a Python class that has methods we must implement, so we can create a custom dataset by creating a subclass that extends the functionality of the Dataset class.\n",
    "\n",
    "To create a custom dataset using PyTorch, we extend the Dataset class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object.\n",
    "\n",
    "We will be using the fashion-MNIST dataset that comes built-in with the torchvision package, so we won’t have to do this for our project. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes.\n",
    "\n",
    "#################################################################################\n",
    "BEHIND THE SCENES:-\n",
    "\n",
    "\n",
    "class OHLC(dataset):\n",
    "        def __init__(self,csv_file):\n",
    "            self.data =pd.read_csv(csv_file)\n",
    "        def __get_item__(self,index):\n",
    "            r = self.data.iloc[index]\n",
    "            label= torch.tensor(r.is_up_day,dtype = torch.long)\n",
    "            sample = self.normalize(torch.tensor([r.open,r.high,r.low,r.close]))\n",
    "            return sample , label\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "#################################################################################            \n",
    "            \n",
    "            \n",
    "All subclasses of the Dataset class must override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch torchvision package\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The torchvision package, gives us access to the following resources:\n",
    "\n",
    "1)Datasets (like MNIST and Fashion-MNIST)\n",
    "2)Models (like VGG16)\n",
    "3)Transforms\n",
    "4)Utils\n",
    "\n",
    "The PyTorch FashionMNIST dataset simply extends the MNIST dataset and overrides the urls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26402816/26421880 [00:44<00:00, 574823.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/29515 [00:00<?, ?it/s]\u001b[A\n",
      "32768it [00:00, 60195.12it/s]                            \u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 16384/4422102 [00:00<00:40, 108358.38it/s]\u001b[A\n",
      "  1%|          | 40960/4422102 [00:00<00:34, 126117.14it/s]\u001b[A\n",
      "  2%|▏         | 98304/4422102 [00:00<00:27, 157803.76it/s]\u001b[A\n",
      "  4%|▍         | 196608/4422102 [00:01<00:22, 185451.80it/s]\u001b[A\n",
      "  7%|▋         | 327680/4422102 [00:01<00:16, 242897.16it/s]\u001b[A\n",
      " 11%|█▏        | 507904/4422102 [00:01<00:12, 305634.10it/s]\u001b[A\n",
      " 14%|█▍        | 614400/4422102 [00:01<00:10, 379647.06it/s]\u001b[A\n",
      " 17%|█▋        | 745472/4422102 [00:01<00:07, 481164.54it/s]\u001b[A\n",
      " 19%|█▉        | 835584/4422102 [00:01<00:06, 546293.97it/s]\u001b[A\n",
      " 21%|██        | 917504/4422102 [00:01<00:06, 567638.78it/s]\u001b[A\n",
      " 23%|██▎       | 999424/4422102 [00:02<00:08, 424790.77it/s]\u001b[A\n",
      " 25%|██▌       | 1114112/4422102 [00:02<00:06, 491352.03it/s]\u001b[A\n",
      " 28%|██▊       | 1236992/4422102 [00:02<00:05, 560863.29it/s]\u001b[A\n",
      " 30%|██▉       | 1310720/4422102 [00:02<00:05, 572009.29it/s]\u001b[A\n",
      " 31%|███▏      | 1384448/4422102 [00:02<00:05, 607297.33it/s]\u001b[A\n",
      " 33%|███▎      | 1458176/4422102 [00:02<00:04, 593395.75it/s]\u001b[A\n",
      " 34%|███▍      | 1523712/4422102 [00:03<00:05, 564063.92it/s]\u001b[A\n",
      " 36%|███▌      | 1589248/4422102 [00:03<00:05, 535359.19it/s]\u001b[A\n",
      " 37%|███▋      | 1654784/4422102 [00:03<00:05, 506954.61it/s]\u001b[A\n",
      " 39%|███▉      | 1744896/4422102 [00:03<00:05, 531049.87it/s]\u001b[A\n",
      " 41%|████▏     | 1835008/4422102 [00:03<00:04, 549633.76it/s]\u001b[A\n",
      " 44%|████▎     | 1925120/4422102 [00:03<00:04, 563572.17it/s]\u001b[A\n",
      " 46%|████▌     | 2015232/4422102 [00:03<00:03, 622447.83it/s]\u001b[A\n",
      " 47%|████▋     | 2097152/4422102 [00:03<00:03, 653777.02it/s]\u001b[A\n",
      " 49%|████▉     | 2170880/4422102 [00:04<00:03, 633858.67it/s]\u001b[A\n",
      " 51%|█████     | 2236416/4422102 [00:04<00:03, 582448.62it/s]\u001b[A\n",
      " 52%|█████▏    | 2301952/4422102 [00:04<00:03, 543522.78it/s]\u001b[A\n",
      " 54%|█████▍    | 2383872/4422102 [00:04<00:03, 553778.39it/s]\u001b[A\n",
      " 56%|█████▌    | 2465792/4422102 [00:04<00:03, 551419.11it/s]\u001b[A\n",
      " 58%|█████▊    | 2555904/4422102 [00:04<00:03, 561270.84it/s]\u001b[A\n",
      " 60%|█████▉    | 2637824/4422102 [00:04<00:03, 557234.93it/s]\u001b[A\n",
      " 62%|██████▏   | 2727936/4422102 [00:05<00:02, 568570.71it/s]\u001b[A\n",
      " 64%|██████▎   | 2809856/4422102 [00:05<00:02, 560320.47it/s]\u001b[A\n",
      " 66%|██████▌   | 2908160/4422102 [00:05<00:02, 582835.62it/s]\u001b[A\n",
      " 68%|██████▊   | 2990080/4422102 [00:05<00:02, 571974.16it/s]\u001b[A\n",
      " 70%|██████▉   | 3080192/4422102 [00:05<00:02, 575950.26it/s]\u001b[A\n",
      " 72%|███████▏  | 3162112/4422102 [00:05<00:02, 569242.93it/s]\u001b[A\n",
      " 74%|███████▎  | 3260416/4422102 [00:06<00:01, 588909.44it/s]\u001b[A\n",
      " 76%|███████▌  | 3350528/4422102 [00:06<00:01, 593784.94it/s]\u001b[A\n",
      " 78%|███████▊  | 3440640/4422102 [00:06<00:01, 594322.45it/s]\u001b[A\n",
      " 80%|████████  | 3538944/4422102 [00:06<00:01, 607605.38it/s]\u001b[A\n",
      " 82%|████████▏ | 3629056/4422102 [00:06<00:01, 604298.60it/s]\u001b[A\n",
      " 84%|████████▍ | 3727360/4422102 [00:06<00:01, 616934.98it/s]\u001b[A\n",
      " 86%|████████▋ | 3817472/4422102 [00:06<00:00, 610485.18it/s]\u001b[A\n",
      " 88%|████████▊ | 3907584/4422102 [00:07<00:00, 606397.39it/s]\u001b[A\n",
      " 90%|█████████ | 3997696/4422102 [00:07<00:00, 604118.26it/s]\u001b[A\n",
      " 92%|█████████▏| 4087808/4422102 [00:07<00:00, 665839.80it/s]\u001b[A\n",
      " 94%|█████████▍| 4161536/4422102 [00:07<00:00, 662606.82it/s]\u001b[A\n",
      " 96%|█████████▌| 4235264/4422102 [00:07<00:00, 615950.89it/s]\u001b[A\n",
      " 97%|█████████▋| 4300800/4422102 [00:07<00:00, 624939.60it/s]\u001b[A\n",
      " 99%|█████████▊| 4366336/4422102 [00:07<00:00, 633761.23it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "8192it [00:00, 22746.98it/s]            \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "26427392it [01:00, 574823.97it/s]                              \n",
      "4423680it [00:24, 633761.23it/s]                             \u001b[A"
     ]
    }
   ],
   "source": [
    "#To get an instance of the FashionMNIST dataset using torchvision, we just create one like so:\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that the root argument used to be './data/FashionMNIST', however, it has since changed due to torchvision updates.\n",
    "\n",
    "\n",
    "Parameter\t                        Description\n",
    "root\t                   The location on disk where the data is located.\n",
    "train\t                   If the dataset is the training set\n",
    "download\t               If the data should be downloaded.\n",
    "transform\t               A composition of transformations that should be performed on the dataset elements.\n",
    "\n",
    "\n",
    "Since we want our images to be transformed into tensors, we use the built-in transforms.ToTensor() transformation, and since this dataset is going to be used for training, we’ll name the instance train_set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD using PyTorch DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set\n",
    "    ,batch_size=1000\n",
    "    ,shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We just pass train_set as an argument. Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand:\n",
    "\n",
    "batch_size (1000 in our case)\n",
    "shuffle (True in our case)\n",
    "num_workers (Default is 0 which means the main process will be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
