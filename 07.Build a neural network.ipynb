{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer in a neural network has two primary components:\n",
    "\n",
    "1. A transformation (code)\n",
    "2. A collection of weights (data or attributes)\n",
    "\n",
    "So, a layer can be represented by a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural networks and layers in PyTorch extend the nn.Module class. This means that we must extend the nn.Module class when building a new layer or neural network in PyTorch.\n",
    "\n",
    "### PyTorch nn.Modules have a forward() method\n",
    "The tensor input is passed forward though the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer has its own transformation (code) and the tensor passes forward through each layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network.\n",
    "\n",
    "The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction.\n",
    "\n",
    "What this all means is that, every PyTorch nn.Module has a forward() method, and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation.\n",
    "\n",
    "When we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps :- \n",
    "1. Create a neural network class that extends the nn.Module base class.\n",
    "2. In the class constructor, define the network’s layers as class attributes using pre-built layers from torch.nn.\n",
    "3. Use the network’s layer attributes as well as operations from the nn.functional API to define the network’s forward    pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch neural network\n",
    "class Network(nn.Module): # class extends the nn.Module class \n",
    "    def __init__(self):\n",
    "        super().__init__() #calls the init function of super class \n",
    "        self.layer = None   #single dummy layer \n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.layer(t)#the forward() function takes in a tensor t and transforms it using the dummy layer. \n",
    "        return t #After the tensor is transformed, the new tensor is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At the moment, our Network class has a single dummy layer as an attribute. \n",
    "#Let’s replace this now with some real layers that come pre-built for us from PyTorch's nn library\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)#linear, dense, and fully connected layer all are same\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t\n",
    " #def __repr__(self):\n",
    "        #return (\"AYUSH RANJAN\"  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the parameters of the layers in PyTorch\n",
    "\n",
    "Each of our layers extends PyTorch's neural network Module class. For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor.\n",
    "\n",
    "The weight tensor inside each layer contains the weight values that are updated as the network learns during the training process, and this is the reason we are specifying our layers as attributes inside our Network class.\n",
    "\n",
    "PyTorch's neural network Module class keeps track of the weight tensors inside each layer. The code that does this tracking lives inside the nn.Module class, and since we are extending the neural network module class, we inherit this functionality automatically.\n",
    "\n",
    "### CNN Layer Parameters\n",
    "\n",
    "Parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function.\n",
    "types of parameters that we used when constructing our layers:\n",
    "\n",
    "1. Hyperparameters\n",
    "2. Data dependent hyperparameters\n",
    "\n",
    "When we construct a layer, we pass values for each parameter to the layer’s constructor. With our convolutional layers have three parameters and the linear layers have two parameters.\n",
    "\n",
    "Convolutional layers\n",
    "1. in_channels\n",
    "2. out_channels\n",
    "3. kernel_size\n",
    "\n",
    "\n",
    "Linear layers\n",
    "1. in_features\n",
    "2. out_features\n",
    "\n",
    "\n",
    "Let's see how the values for the parameters are decided. We'll start by looking at hyperparameters, and then, we'll see how the dependent hyperparameters fall into place.\n",
    "\n",
    "### Hyperparameter \n",
    "hyperparameters are parameters whose values are chosen manually and arbitrarily.we choose hyperparameter values mainly based on trial and error and increasingly by utilizing values that have proven to work well in the past\n",
    "\n",
    "our CNN layers, these are the parameters we choose manually.\n",
    "\n",
    "1. kernel_size\n",
    "2. out_channels\n",
    "3. out_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parameter\t                      Description\n",
    "kernel_size\t                Sets the filter size. The words kernel and filter are interchangeable.\n",
    "out_channels\t            Sets the number of filters. One filter produces one output channel.\n",
    "out_features\t            Sets the size of the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dependent hyperparameters\n",
    "\n",
    "Data dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent hyperparameters that stick out are the in_channels of the first convolutional layer, and the out_features of the output layer.\n",
    "\n",
    "You see, the in_channels of the first convolutional layer depend on the number of color channels present inside the images that make up the training set. Since we are dealing with grayscale images, we know that this value should be a 1.\n",
    "\n",
    "The out_features for the output layer depend on the number of classes that are present inside our training set. Since we have 10 classes of clothing inside the Fashion-MNIST dataset, we know that we need 10 output features.\n",
    "\n",
    "In general, the input to one layer is the output from the previous layer, and so all of the in_channels in the conv layers and in_features in the linear layers depend on the data coming from the previous layer.\n",
    "\n",
    "\n",
    "When we switch from a conv layer to a linear layer, we have to flatten our tensor. This is why we have 12*4*4. The twelve comes from the number of output channels in the previous layer, but why do we have the two 4s?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Layer\t            Param name\t        Param value\t                           The param value is\n",
    "conv1\t            in_channels\t            1\t               the number of color channels in the input image.\n",
    "conv1\t            kernel_size\t            5\t                               a hyperparameter.\n",
    "conv1\t            out_channels\t        6\t                               a hyperparameter.\n",
    "conv2\t            in_channels\t            6\t               the number of out_channels in previous layer.\n",
    "conv2\t            kernel_size\t            5\t                               a hyperparameter.\n",
    "conv2\t            out_channels\t        12\t             a hyperparameter(always higher than previous conv layer).\n",
    "fc1\t                in_features\t          12*4*4\t         the length of the flattened output from previous layer.\n",
    "fc1\t                out_features\t       120\t                                a hyperparameter.\n",
    "fc2\t                in_features\t           120\t             the number of out_features of previous layer.\n",
    "fc2\t                out_features\t       60\t              a hyperparameter (lower than previous linear layer).\n",
    "out\t                in_features\t           60\t              the number of out_channels in previous layer.\n",
    "out\t                out_features\t       10\t                  the number of prediction classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Weights - Learnable Parameters in Neural Networks\n",
    "Learnable parameters are parameters whose values are learned during the training process.\n",
    "\n",
    "In fact, when we say that a network is learning, we specifically mean that the network is learning the appropriate values for the learnable parameters. Appropriate values are values that minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting an Instance the Network\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)\n",
    "#The print() function prints to the console a string representation of our network. \n",
    "#With a sharp eye,\n",
    "#we can notice that the printed output here is detailing our network’s architecture listing out our network’s layers, \n",
    "#and showing the values that were passed to the layer constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Network object at 0x12533c400>\n"
     ]
    }
   ],
   "source": [
    "print(network)\n",
    "#if network was a normal class and hasn't inherited the nn.module class print would give us object definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can override Python’s default string representation using the __repr__ function. This name is short for representation. And this is what happened when we made Network a child of nn.Module class. nn.module class has overridden the default __rep__ to the layers detail .\n",
    "\n",
    "\n",
    "Let's try to change the output  for the above case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "    return \"AYUSH RANJAN\"\n",
    "#this above code is copied into the class whose object we made ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AYUSH RANJAN\n"
     ]
    }
   ],
   "source": [
    "print(network)\n",
    "#Now the output is what ever we want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coming back to where we were ..\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the convolutional layers, the kernel_size argument is a Python tuple (5,5) even though we only passed the number 5 in the constructor.\n",
    "                   This is because our filters actually have a height and width, and when we pass a single number, the code inside the layer’s constructor assumes that we want a square filter.\n",
    "                   \n",
    "                   \n",
    "The stride tells the conv layer how far the filter should slide after each operation in the overall convolution. This tuple says to slide by one unit when moving to the right and also by one unit when moving down.\n",
    "\n",
    "\n",
    "### Accessing the Network's Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=192, out_features=120, bias=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=60, out_features=10, bias=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1020,  0.1215, -0.1171, -0.1623,  0.1411],\n",
       "          [-0.0527,  0.1480,  0.0073,  0.1955,  0.0518],\n",
       "          [-0.1070, -0.0038, -0.1312, -0.1623,  0.1520],\n",
       "          [ 0.1162,  0.0866, -0.1269,  0.1381,  0.0608],\n",
       "          [-0.1013,  0.1447,  0.1565, -0.0279, -0.1306]]],\n",
       "\n",
       "\n",
       "        [[[-0.1022,  0.0904,  0.1983,  0.0466,  0.0404],\n",
       "          [-0.0107, -0.1948,  0.0105, -0.1969, -0.1783],\n",
       "          [-0.1969,  0.1460,  0.1940,  0.1031,  0.1218],\n",
       "          [-0.1357,  0.1991, -0.1208, -0.0384,  0.1914],\n",
       "          [ 0.1424,  0.1173,  0.0513,  0.1940, -0.1058]]],\n",
       "\n",
       "\n",
       "        [[[-0.1823,  0.0417,  0.0566, -0.0692, -0.0958],\n",
       "          [-0.1608, -0.0551, -0.0730,  0.1810,  0.0611],\n",
       "          [-0.1811,  0.1298, -0.1511,  0.1434,  0.0097],\n",
       "          [-0.0723, -0.0159,  0.1773,  0.1791, -0.0224],\n",
       "          [ 0.0687,  0.1868, -0.1445,  0.0323,  0.1713]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1980,  0.1130,  0.1308,  0.1784,  0.0372],\n",
       "          [-0.0367, -0.0877,  0.1149,  0.0960,  0.1227],\n",
       "          [ 0.1352, -0.0780, -0.0622,  0.1372, -0.1060],\n",
       "          [ 0.1834,  0.0780, -0.1131, -0.1309,  0.1898],\n",
       "          [-0.1851,  0.1709, -0.0122,  0.1596,  0.1843]]],\n",
       "\n",
       "\n",
       "        [[[-0.0211, -0.0331, -0.0948, -0.1513, -0.1253],\n",
       "          [ 0.1708,  0.1298,  0.0573, -0.1453, -0.1320],\n",
       "          [-0.1363, -0.0098,  0.1301, -0.1011,  0.1320],\n",
       "          [ 0.0012,  0.0489, -0.1580,  0.0943,  0.1199],\n",
       "          [-0.1632, -0.0208, -0.0691,  0.1854, -0.0283]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1283, -0.0015, -0.0827,  0.1863, -0.0528],\n",
       "          [ 0.0259,  0.0210, -0.1870,  0.1928, -0.1868],\n",
       "          [ 0.1306, -0.1182,  0.1441, -0.0532, -0.1884],\n",
       "          [-0.1288, -0.0599, -0.1804, -0.1485, -0.0748],\n",
       "          [-0.1447, -0.1029, -0.1951,  0.1752,  0.1258]]]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output is a tensor, but before we look at the tensor, let’s talk OOP for a moment. This is a good example that showcases how objects are nested. We first access the conv layer object that lives inside the network object.Then, we access the weight tensor object that lives inside the conv layer object, so all of these objects are chained or linked together\n",
    "\n",
    "\n",
    "One thing to notice about the weight tensor output is that it says parameter containing at the top of the output. This is because this particular tensor is a special tensor because its values or scalar components are learnable parameters of our network.\n",
    "\n",
    "To keep track of all the weight tensors inside the network. PyTorch has a special class called Parameter. The Parameter class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class. This is why we see the Parameter containing text at the top of the string representation output.\n",
    "\n",
    "We can see in the Pytorch source code that the Parameter class is overriding the __repr__ function by prepending the text parameter containing to the regular tensor class representation output.\n",
    "\n",
    "# Weight Tensor Shape\n",
    "\n",
    "For the convolutional layers, the weight values live inside the filters,and in code, the filters are actually the weight tensors themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1\n",
    "# we have 1 color channel that should be convolved by 6 filters of size 5x5 to produce 6 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape\n",
    "#The first axis has a length of 6, and this accounts for the 6 filters.\n",
    "#The second axis has a length of 1 which accounts for the single input channel, \n",
    "#and the last two axes account for the height and width of the filter.\n",
    "#This means we are packaging all of our filters into a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv2.weight.shape\n",
    "#there are 6 input channels coming from the previous layer.\n",
    "#Think of this value of 6 here as giving each of the filters some depth. \n",
    "#Instead of having a filter that convolves all of the channels iteratively, \n",
    "#our filter has a depth that matches the number of channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 points to take away from this :-\n",
    "1. All filters are represented using a single tensor.\n",
    "2. Filters have depth that accounts for the input channels.\n",
    "\n",
    "For weights the 4 axis represents:-\n",
    "(Number of filters, Depth, Height, Width)\n",
    "\n",
    "\n",
    "##### With linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a weight matrix.\n",
    "This is due to the fact that the weight tensor is of rank-2 with height and width axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc2.weight.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out.weight.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " height of the weight tensor has the length of the desired output features and a width of the input features.\n",
    " \n",
    " # Accessing the Networks Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in network.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t \t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t \t torch.Size([6])\n",
      "conv2.weight \t \t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t \t torch.Size([12])\n",
      "fc1.weight \t \t torch.Size([120, 192])\n",
      "fc1.bias \t \t torch.Size([120])\n",
      "fc2.weight \t \t torch.Size([60, 120])\n",
      "fc2.bias \t \t torch.Size([60])\n",
      "out.weight \t \t torch.Size([10, 60])\n",
      "out.bias \t \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, p in network.named_parameters():\n",
    "    print(name,'\\t \\t' ,p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
