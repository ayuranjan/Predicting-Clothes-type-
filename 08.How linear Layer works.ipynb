{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Linear Layers Work\n",
    "linear layers use matrix multiplication to transform their in features to out features.\n",
    "\n",
    "When the input features are received by a linear layer, they are received in the form of a flattened 1-dimensional tensor and are then multiplied by the weight matrix. This matrix multiplication produces the output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how linear layers work as well. They map an in_feature space to an out_feature space using a weight matrix.\n",
    "\n",
    "Let's see how to create a PyTorch linear layer that will do this same operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6724, -1.7043,  2.8615], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)\n",
    "# here we can see when we pass 4 in features it indeed gives us 3 out features as we mentioned .HOW?\n",
    "#PyTorch is using these value to create a matrix of size out_features * in_features i.e. 3*4 \n",
    "#And then multiply the 3 *4 weight matrix with 4 *1 flat input tensor to give 3 *1 output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can call the object instance like this because PyTorch neural network modules are callable Python objects. \n",
    "Different values were produced.\n",
    "\n",
    "This is because PyTorch creates a weight matrix and initializes it with random values\n",
    "\n",
    "Let's explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30.4009, 39.6187, 49.5835], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stil we got values which are not exactly the same as start . WHY?\n",
    "\n",
    "Because Bias is True as a default and bias is added to the product of those two .\n",
    "\n",
    "lets set bias = false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)\n",
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)\n",
    "#Now we got the exact value . This is how linear layers work."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sometimes we'll see linear layer operation referred to as\n",
    "              y=Ax+b\n",
    "\n",
    "Variable\t       Definition\n",
    "\n",
    " A                 Weight matrix tensor\n",
    "\n",
    " x                  Input tensor\n",
    "\n",
    " b                   Bias tensor\n",
    "\n",
    " y                  Output tensor\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callable Layers and Neural Networks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "We pointed out before how it was kind of strange that we called the layer object instance as if it were a function.\n",
    "\n",
    "This fact is an important PyTorch concept because of the way the __call__() method interacts with the forward() method for our layers and networks.\n",
    "\n",
    "Instead of calling the forward() method directly, we call the object instance. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. This applies to all PyTorch neural network modules, namely, networks and layers.\n",
    "\n",
    "Let's see this in the PyTorch source code.\n",
    "\n",
    " \n",
    "\n",
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "    if torch._C._get_tracing_state():\n",
    "        result = self._slow_forward(*input, **kwargs)\n",
    "    else:\n",
    "        result = self.forward(*input, **kwargs)\n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "        if hook_result is not None:\n",
    "            raise RuntimeError(\n",
    "                \"forward hooks should never return any values, but '{}'\"\n",
    "                \"didn't return None\".format(hook))\n",
    "    if len(self._backward_hooks) > 0:\n",
    "        var = result\n",
    "        while not isinstance(var, torch.Tensor):\n",
    "            if isinstance(var, dict):\n",
    "                var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "            else:\n",
    "                var = var[0]\n",
    "        grad_fn = var.grad_fn\n",
    "        if grad_fn is not None:\n",
    "            for hook in self._backward_hooks.values():\n",
    "                wrapper = functools.partial(hook, self)\n",
    "                functools.update_wrapper(wrapper, hook)\n",
    "                grad_fn.register_hook(wrapper)\n",
    "    return result\n",
    "The extra code that PyTorch runs inside the __call__() method is why we never invoke the forward() method directly. If we did, the additional PyTorch code would not be executed. As a result, any time we want to invoke our forward() method, we call the object instance. This applies to both layers, and networks because they are both PyTorch neural network modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
